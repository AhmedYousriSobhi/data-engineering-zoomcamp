# Data Storage
## Brief History of data storage
Following info were collected from [source](https://www.dataversity.net/brief-history-data-warehouse/).

Early in the 1950s till mid-1980s, the data were stored in punch cards.
- The warning "Do not fold, spindle, or mutilate" originally came from punch cards.
- ![punch card](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Used_Punchcard_%285151286161%29.jpg/300px-Used_Punchcard_%285151286161%29.jpg)

In 1960s, Megnetic storage slowly replaced punch cards, so Disk storage came as the next evolutionary step for data storage.
- Hard drives & floppies.
- IBM was primarily responsible for the early evolution of disk storage.

In 1966, Disk storage was quickly followed by software called (DBMS) "Database Management System".
- IBM came with its own DBMS, which was called at that time "an information management system".
- DBMS was designed to manage the storage on the disk, which included the following abilities:
    - Identify the proper location of data.
    - Resolve conflicts when more than one unit of data is mapped to the same location.
    - Allow data to be deleted.
    - Find room when stored data won't fit in a specific, limited physical location.
    - Find data quickly (which was the greatest benefit).
    - Online Applications.

In the late 1960s & early 1970s, Commercial online applications came into play, shortly after disk storage and DBMS software became poplular.
- Once it was realized data could be accessed directly, information began being shared between computers. As a result, there were a large number of commercial applications which could be applied to online processing. Some examples included:
    - Claims processing
    - Bank teller processing
    - Automated teller processing (ATMs)
    - Airline reservation processing
    - Retail point-of-sale processing
    - Manufacturing control processing

Inspite of these improvements, finding specific data could be difficult, and it was not necessarily trustworthly.
- The data found might be based on 'old' information.
- Much data was being generated by corporations that people couldn't trust the accuracy of that data.

In late 1970s, Personal Computers and 4GL Technology. which lead to free end user, allowing them to access their own data.
- Personal computer technology lets anyone bring their computer to work and do processing when convenient, and they store their "personal" data on their computer.
    - Due to that, It was actually a though that centralized IT departments are no longer needed.
- Simultaneously, a technology called Fourth Generation Programming Language '4GL' was developed and promoted.
    - 4GLs are designed to reduce the overall time, effort and cost of software development. The main domains and families of 4GLs are: database queries, report generators, data manipulation, analysis and reporting, screen painters and generators, GUI creators, mathematical optimization, web developmentand general purpose languages.
    - Also this technology prompted the disintegration of centralized IT department.
- But along the way, end useres discovered that:
    - Incorrect data can be misleading.
    - Incomplete data may not be very useful.
    - Old data is not desirable.
    - Multiple versions of the same data can be confusing.
    - Data lacking documentation is questionable.

In the 1980s, Relational Databases became popular, they were user-friendly.
- SQL is the language used by RDBMS "Relational DBMS".
- By late 1980s, Many businesses had moved away from mainframe computers. Staff members were now assigned a personal computer, and office applications (Excel, Word, Access) started gaining favor.

The Need for Data warehouses
- During the 1990s major cultural and technological changes were taking place.
- The internet was surging in popularity.
- Competition had increased due to new free trade agreements, computerization, globalization, and networking.
- This new reality required greater business intelligence, resulting in the nedd for true data warehousing.
- By the year 2000, many businesses discovered that, with the expansion of databases and application systems, their systems had been badly integrated and that their data was inconsistent. They discovered that they were receiving and storing lots of fragments of data. Somehow, the data needed to be integrated to provide the critical business information nedded for decision-making in a competitive global economy.
- Data warehouses were developed by businesses to consolidate the data they were taking from a variety of databases and to help support their strategic decision-making efforts.

## Key Features of Data Warehouse
- A repository for data generated or collected by business applications, and then stored for a predetermined analytics purpose.
- Are build on relational database, as a results, they do apply a predefined schema to data.
- Data must be cleansed, consolidated, and organized for the intended uses before being loaded.

# Data Warehouse Alternatives
## __Data Lake__
in addition to data lakehouses, have recently gained popularity. Data lakes use a more flexible structure for collecting and storing data than a data warehouse. Data lakes preserve the original structure of data and can be used as a retrieval and storage system for big data, which could, theoretically, scale upward indefinitely. (The term “big data” is dropping out of use, because, these days, big data is normal and no longer “big.”)
### Key Features
- A vast repository that stores raw data in its native format.
- Can store data of varting structures, not just traditional structured data.
- Each stored data elements is tagged with a unique identifier and metadata, so it can be queried more easily when needed.
- Data lakes don't require a predefined schema when data is ingested. instead, data scientists and other analysts can apply a schema to data sets and filter them for specific analytics after the ingestion process is complete.
![data lake architecture example](https://cdn.ttgtmedia.com/rms/onlineimages/example_of_a_data_lake_architecture-f.png)

### Data Lake Vs War3house
![Lake Vs War3house](https://www.qubole.com/wp-content/uploads/2020/12/Dl-vs-DW-infograph-1000x563.png)

![lake vs warehouse](https://panoply.io/uploads/data-warehouse-vs-data-lake-2.png)

### __Cloud Providers__ for Data Lake
- GCP - cloud storage.
- AWS - S3.
- AZURE - AZURE BLOB.

## __Data Mart__
A __data mart__ is an area for storing data that serves a particular community or group of workers. It is a storage area with fixed data and is deliberately under the control of one department within the organization.

## __Data Cube__
A __data cube__ is software that stores data in matrices of three or more dimensions. Any transformations in the data are expressed as tables and arrays of processed information. After tables have matched the rows of data strings with the columns of data types, the data cube then cross-references tables from a single data source or multiple data sources, increasing the detail of each data point. This arrangement provides researchers with the ability to find deeper insights than other techniques.

## __Data Swamps__
__Data swamps__ can result from a poorly designed or neglected data lake. A data swamp describes the failures to document stored data correctly. This situation makes the data difficult to analyze and use efficiently. While the original data may still be there, a data swamp cannot recover it without the appropriate metadata for context.

Data Lake containing unstructed, ungoverned data, that has gotten out of hand.
- For example, Storing data in specific format, then chagning the format and continue storing in same folder location. That makes it hard for consumers to consume it, then the data become useless.

Result of lack of process and standars.
    - No Versioning.
    - Incompatible shcemas for same data without Versioning.
    - No metadata associated.
    - Diffuclt to find, manuipulate, and inevitably-analyze.

## __Data Silo__
__Data silos__ can naturally occur in large organizations, with each department having different goals, responsibilities, and priorities. Data silos are storage areas of fixed data that are under the control of a single department and have been separated and isolated from access by other departments for privacy and security. Data silos can also happen when departments compete instead of working together towards common goals. They are generally considered a hindrance to collaboration and efficient business practices.

A collection of data that is isolated from other data within an organization.

This could happen for a variety of reasons, such as:
- __Different departments__ using different software systems.
- Data being stored in __different formats__.
- Data being stored in __different locations__.
- Data being __owned by different teams or individuals__.

Data silos can have a number of __negative consequences__ for an organization, including:
- __Reduced efficiency__: Data silos can make it difficult to share data between departments, which can lead to duplication of effort and wasted time.
- __Reduced agility__: Data silos can make it difficult to respond to changes in the market or customer demands.
- __Reduced innovation__: Data silos can make it difficult to identify new opportunities or develop new products and services.
- __Increased risk__: Data silos can make it more difficult to comply with regulations and protect data from security breaches.

There are a number of things that organizations can do to address the problem of data silos, such as:
- Standardizing data formats and processes
- Investing in data integration tools
- Creating a data governance framework
- Promoting a culture of data sharing

By taking these steps, organizations can break down data silos and unlock the value of their data.

Here are some of the challenges of data silos:
- __Data duplication__: When data is siloed, it is often duplicated across different systems. This can lead to data inconsistencies and errors.
- __Data fragmentation__: Data silos can lead to data fragmentation, which means that data is stored in different formats and locations. This can make it difficult to access and analyze data.
- __Data security__: Data silos can make it difficult to protect data from security breaches. This is because data is often stored in different systems and locations, which can make it difficult to track and monitor.
- __Data governance__: Data silos can make it difficult to implement data governance policies and procedures. This is because data is often owned by different teams or departments, which can lead to conflicting priorities and goals.

Here are some of the benefits of breaking down data silos:
- __Improved decision-making__: When data is siloed, it can be difficult to get a complete picture of the business. This can make it difficult to make informed decisions.
- __Increased efficiency__: When data is siloed, it can be difficult to share data between departments. This can lead to duplication of effort and wasted time.
Improved customer service: When data is siloed, it can be difficult to provide personalized customer service. This can lead to customer dissatisfaction and churn.
- __Increased innovation__: When data is siloed, it can be difficult to identify new opportunities or develop new products and services.
- __Reduced risk__: When data is siloed, it can be more difficult to comply with regulations and protect data from security breaches.

If you are looking to break down data silos in your organization, there are a number of steps you can take. These include:
- __Standardizing data formats and processes__: This will make it easier to share data between systems.
- __Investing in data integration tools__: This will help you to connect different systems and bring data together into a single view.
- __Creating a data governance framework__: This will help you to define roles and responsibilities for managing data.
- __Promoting a culture of data sharing__: This will help to break down silos and encourage people to share data.

# Data Orchestration
To fully understand how we got to this concept of Orchestration, let's go back a little bit to understand what were there.

## Hirtory of word Orchestration
The word orchestration in data orchestration comes from the classical music definition of orchestration, which is the study or practice of writing music for an orchestra. In data orchestration, the same principles are applied to the automated configuration, coordination, and management of computer systems and software. Just as a conductor leads an orchestra, a data orchestrator leads a team of data engineers and architects in the process of collecting, transforming, and delivering data to users.

The term data orchestration was first coined in the early 2000s, as businesses began to move away from traditional data warehouses and towards more agile and scalable data architectures. In these new architectures, data is often stored in a variety of different locations, including on-premises, in the cloud, and in third-party data lakes. This can make it difficult to manage and analyze data, which is where data orchestration comes in.

Here are some of the benefits of data orchestration:
- __Improved data quality__: Data orchestration platforms can help to improve data quality by ensuring that data is consistently formatted and validated.
- __Increased data agility__: Data orchestration platforms can help to increase data agility by making it easier to move data between different storage locations and applications.
- __Better decision-making__: Data orchestration platforms can help businesses to make better decisions based on data by providing them with a single view of their data.


## Workflow Orchestration    
- Means govering your data flow in a way that respects orchestration rules and your business logic.

- Orchestration is the coordination and management of multiple computer systems, applications and/or services, stringing together multiple tasks in order to execute a larger workflow or process

- Workflow: a system for managing repetitive processes and tasks which occur in a particular order.

- Workflow orchestration tool is going to allow you to turn any code into a workflow that you can schedule run and observe.

     - Like Apache Airflow, Prefect.

        Apache Airflow:    
        - 2015, by AirBnb
        - Based on core concept of Directed Acyclic Graph (DAG)
                
        ![image](https://cloud.google.com/static/composer/docs/images/workflow-group-dags.png)

        - DAG is a python script, that contains all these controllers functions.
        - Each task is a python script, performs some function on the data, such as cleaning, organizing, ingesting, computing metrics, ... etc.
        
        ![image](https://github.com/AhmedYousriSobhi/DeepLearning.AI-Deep-Learning-Specialization/assets/66730765/eeda5dd6-24a3-46d8-ab55-4c3c4b0d6e6a)
